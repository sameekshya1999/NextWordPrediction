{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0lxmTYWbStbwBmHuKxKzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameekshya1999/NextWordPrediction/blob/main/GRU_nextword%20%20fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UbruiKIJ6lnf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding,Dense,GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Load your file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CRLL_Zee7DcV",
        "outputId": "7b91ee5f-a10b-47f4-e7f1-fb59493d298e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-03c8bf20-1367-4169-a84f-7ac71e183861\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-03c8bf20-1367-4169-a84f-7ac71e183861\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cinderella.txt to cinderella (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Open and pre-process the data\n",
        "file = open(\"cinderella.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)\n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oLJ3VkJH7HYQ",
        "outputId": "45530e31-f082-44ac-e345-c32f2f54ed7a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cinderella: A Tale of Hope and Kindness Once upon a time, in a faraway land, there was a kind and ge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3G5-cvB7KgI",
        "outputId": "14671af6-c331-4b24-e2dd-026909112c57"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5704"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zws4aYcI7NtM",
        "outputId": "5e89729b-041d-4f11-a048-8a96c73571a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 7, 131, 11, 132, 3, 67, 133, 134, 7, 68, 9, 7, 135, 136]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2u4Tnro7RO7",
        "outputId": "c0ec1458-e797-4201-82ba-34bdace9625c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "972"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkntWu5T7Trx",
        "outputId": "1857c2c6-151b-4989-cc1e-3e3bb68dca41"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXIeiVaj7WSn",
        "outputId": "2b737a85-e4e9-4870-f6c4-62e0c568a33b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  969\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6,   7, 131,  11],\n",
              "       [  7, 131,  11, 132],\n",
              "       [131,  11, 132,   3],\n",
              "       [ 11, 132,   3,  67],\n",
              "       [132,   3,  67, 133],\n",
              "       [  3,  67, 133, 134],\n",
              "       [ 67, 133, 134,   7],\n",
              "       [133, 134,   7,  68],\n",
              "       [134,   7,  68,   9],\n",
              "       [  7,  68,   9,   7]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plxHxzBZ7Y2C",
        "outputId": "ab230fa5-fd9a-4abf-a3e8-a6f7651a3c6d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[  6   7 131]\n",
            " [  7 131  11]\n",
            " [131  11 132]\n",
            " [ 11 132   3]\n",
            " [132   3  67]\n",
            " [  3  67 133]\n",
            " [ 67 133 134]\n",
            " [133 134   7]\n",
            " [134   7  68]\n",
            " [  7  68   9]]\n",
            "Response:  [ 11 132   3  67 133 134   7  68   9   7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIM8iQFZ7bQa",
        "outputId": "e048e190-5851-4fe3-8d9a-399c759b6847"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=30)"
      ],
      "metadata": {
        "id": "RjRdX08w7dwn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dropout, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(3, )))  # Define input shape here\n",
        "model.add(Embedding(vocab_size, 10))\n",
        "model.add(GRU(1000, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(1000))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "GbDe0Mq77gI1",
        "outputId": "2142b954-e4b8-4181-97c8-4048fc800657"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │           \u001b[38;5;34m3,790\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_6 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1000\u001b[0m)             │       \u001b[38;5;34m3,036,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1000\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_7 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)                │       \u001b[38;5;34m6,006,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)                │       \u001b[38;5;34m1,001,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m379\u001b[0m)                 │         \u001b[38;5;34m379,379\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,790</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,036,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,006,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">379</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">379,379</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,426,169\u001b[0m (39.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,426,169</span> (39.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,426,169\u001b[0m (39.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,426,169</span> (39.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Define the GRU model\n",
        "gru_model = Sequential([\n",
        "    GRU(64, input_shape=(512, 1)),  # Adjust units and input_shape as needed\n",
        "    Dense(1)  # Add the final dense layer for output, adjust as needed\n",
        "])\n",
        "\n",
        "# Plot the model\n",
        "plot_model(gru_model, to_file='plot1.png', show_layer_names=True, show_shapes=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "pP1MhNi07jP5",
        "outputId": "e206a2e7-8fde-4cb9-9181-eff7cfe10cd5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the GRU model\n",
        "gru_model = Sequential([\n",
        "    GRU(64, input_shape=(512, 1)),  # Adjust units and input_shape as needed\n",
        "    Dense(379, activation='softmax')  # Set the output units to the number of classes with softmax activation\n",
        "])\n",
        "\n",
        "# Compile the model with categorical_crossentropy\n",
        "checkpoint = ModelCheckpoint(\"next_words_gru.keras\", monitor='loss', verbose=1, save_best_only=True)\n",
        "gru_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "gru_model.fit(X_train, y_train, epochs=400, batch_size=50, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUjjQt3G8GEY",
        "outputId": "588d8e11-f5d7-41dd-dc1f-aa4790e8939d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0202 - loss: 5.9867       \n",
            "Epoch 1: loss improved from inf to 5.91222, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0279 - loss: 5.9657\n",
            "Epoch 2/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0564 - loss: 5.6597 \n",
            "Epoch 2: loss improved from 5.91222 to 5.60688, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0557 - loss: 5.6475\n",
            "Epoch 3/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0547 - loss: 5.3954 \n",
            "Epoch 3: loss improved from 5.60688 to 5.34235, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0549 - loss: 5.3843\n",
            "Epoch 4/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0570 - loss: 5.1563 \n",
            "Epoch 4: loss improved from 5.34235 to 5.13570, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0577 - loss: 5.1437\n",
            "Epoch 5/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0723 - loss: 5.0047 \n",
            "Epoch 5: loss improved from 5.13570 to 5.02004, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0728 - loss: 5.0061\n",
            "Epoch 6/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0736 - loss: 4.9761 \n",
            "Epoch 6: loss improved from 5.02004 to 4.95041, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0733 - loss: 4.9666\n",
            "Epoch 7/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0664 - loss: 4.9266  \n",
            "Epoch 7: loss improved from 4.95041 to 4.89238, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0723 - loss: 4.9106\n",
            "Epoch 8/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0864 - loss: 4.8079 \n",
            "Epoch 8: loss improved from 4.89238 to 4.84340, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0851 - loss: 4.8153\n",
            "Epoch 9/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1032 - loss: 4.7154 \n",
            "Epoch 9: loss improved from 4.84340 to 4.79810, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0995 - loss: 4.7336\n",
            "Epoch 10/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1069 - loss: 4.7231 \n",
            "Epoch 10: loss improved from 4.79810 to 4.75325, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1025 - loss: 4.7291\n",
            "Epoch 11/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0966 - loss: 4.6859 \n",
            "Epoch 11: loss improved from 4.75325 to 4.70928, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0952 - loss: 4.6933\n",
            "Epoch 12/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0929 - loss: 4.6378 \n",
            "Epoch 12: loss improved from 4.70928 to 4.66796, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0918 - loss: 4.6483\n",
            "Epoch 13/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0884 - loss: 4.6143  \n",
            "Epoch 13: loss improved from 4.66796 to 4.62792, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0884 - loss: 4.6184\n",
            "Epoch 14/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0840 - loss: 4.5632 \n",
            "Epoch 14: loss improved from 4.62792 to 4.58842, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0856 - loss: 4.5702\n",
            "Epoch 15/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0801 - loss: 4.5970 \n",
            "Epoch 15: loss improved from 4.58842 to 4.55164, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0829 - loss: 4.5873\n",
            "Epoch 16/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0698 - loss: 4.5546 \n",
            "Epoch 16: loss improved from 4.55164 to 4.51551, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0759 - loss: 4.5438\n",
            "Epoch 17/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0856 - loss: 4.4434 \n",
            "Epoch 17: loss improved from 4.51551 to 4.47923, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0886 - loss: 4.4544\n",
            "Epoch 18/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0830 - loss: 4.4568 \n",
            "Epoch 18: loss improved from 4.47923 to 4.44557, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0870 - loss: 4.4548\n",
            "Epoch 19/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0932 - loss: 4.3957 \n",
            "Epoch 19: loss improved from 4.44557 to 4.41218, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0961 - loss: 4.4021\n",
            "Epoch 20/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0944 - loss: 4.3837 \n",
            "Epoch 20: loss improved from 4.41218 to 4.37755, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0950 - loss: 4.3853\n",
            "Epoch 21/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1119 - loss: 4.3414 \n",
            "Epoch 21: loss improved from 4.37755 to 4.34568, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1088 - loss: 4.3419\n",
            "Epoch 22/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1078 - loss: 4.3463 \n",
            "Epoch 22: loss improved from 4.34568 to 4.31242, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1054 - loss: 4.3406\n",
            "Epoch 23/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1001 - loss: 4.2789 \n",
            "Epoch 23: loss improved from 4.31242 to 4.27992, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1004 - loss: 4.2772\n",
            "Epoch 24/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1172 - loss: 4.2156 \n",
            "Epoch 24: loss improved from 4.27992 to 4.25158, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1126 - loss: 4.2250\n",
            "Epoch 25/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1092 - loss: 4.2285 \n",
            "Epoch 25: loss improved from 4.25158 to 4.22269, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1076 - loss: 4.2254\n",
            "Epoch 26/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1056 - loss: 4.1089 \n",
            "Epoch 26: loss improved from 4.22269 to 4.19278, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1048 - loss: 4.1354\n",
            "Epoch 27/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0890 - loss: 4.1577 \n",
            "Epoch 27: loss improved from 4.19278 to 4.16648, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0952 - loss: 4.1621\n",
            "Epoch 28/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1170 - loss: 4.0912 \n",
            "Epoch 28: loss improved from 4.16648 to 4.13895, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1128 - loss: 4.1049\n",
            "Epoch 29/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1299 - loss: 4.0557 \n",
            "Epoch 29: loss improved from 4.13895 to 4.11124, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1233 - loss: 4.0709\n",
            "Epoch 30/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1140 - loss: 4.0199 \n",
            "Epoch 30: loss improved from 4.11124 to 4.08411, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1124 - loss: 4.0335\n",
            "Epoch 31/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1145 - loss: 4.0806 \n",
            "Epoch 31: loss improved from 4.08411 to 4.05907, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1086 - loss: 4.0707\n",
            "Epoch 32/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1053 - loss: 4.0062  \n",
            "Epoch 32: loss improved from 4.05907 to 4.03413, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1083 - loss: 4.0178\n",
            "Epoch 33/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1274 - loss: 3.9934  \n",
            "Epoch 33: loss improved from 4.03413 to 4.00693, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1228 - loss: 3.9968\n",
            "Epoch 34/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0975 - loss: 4.0010 \n",
            "Epoch 34: loss improved from 4.00693 to 3.98352, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1034 - loss: 3.9981\n",
            "Epoch 35/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1255 - loss: 3.9452 \n",
            "Epoch 35: loss improved from 3.98352 to 3.96056, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1239 - loss: 3.9469\n",
            "Epoch 36/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1110 - loss: 3.9232  \n",
            "Epoch 36: loss improved from 3.96056 to 3.93774, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1100 - loss: 3.9309\n",
            "Epoch 37/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1282 - loss: 3.8821 \n",
            "Epoch 37: loss improved from 3.93774 to 3.91484, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1240 - loss: 3.8893\n",
            "Epoch 38/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1149 - loss: 3.8661 \n",
            "Epoch 38: loss improved from 3.91484 to 3.89007, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1153 - loss: 3.8718\n",
            "Epoch 39/400\n",
            "\u001b[1m 7/17\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1138 - loss: 3.9037  \n",
            "Epoch 39: loss improved from 3.89007 to 3.86815, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1144 - loss: 3.8892\n",
            "Epoch 40/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1055 - loss: 3.9283  \n",
            "Epoch 40: loss improved from 3.86815 to 3.84633, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1077 - loss: 3.9109\n",
            "Epoch 41/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1217 - loss: 3.8386 \n",
            "Epoch 41: loss improved from 3.84633 to 3.82539, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1212 - loss: 3.8309\n",
            "Epoch 42/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0905 - loss: 3.8721  \n",
            "Epoch 42: loss improved from 3.82539 to 3.80612, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0948 - loss: 3.8578\n",
            "Epoch 43/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1160 - loss: 3.7320 \n",
            "Epoch 43: loss improved from 3.80612 to 3.78157, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1167 - loss: 3.7426\n",
            "Epoch 44/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1090 - loss: 3.7465 \n",
            "Epoch 44: loss improved from 3.78157 to 3.76053, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1111 - loss: 3.7500\n",
            "Epoch 45/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1067 - loss: 3.7242 \n",
            "Epoch 45: loss improved from 3.76053 to 3.74004, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1101 - loss: 3.7301\n",
            "Epoch 46/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1306 - loss: 3.6698 \n",
            "Epoch 46: loss improved from 3.74004 to 3.72119, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1291 - loss: 3.6845\n",
            "Epoch 47/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1156 - loss: 3.7598 \n",
            "Epoch 47: loss improved from 3.72119 to 3.70127, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1180 - loss: 3.7378\n",
            "Epoch 48/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1193 - loss: 3.7081  \n",
            "Epoch 48: loss improved from 3.70127 to 3.68010, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1237 - loss: 3.6983\n",
            "Epoch 49/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1552 - loss: 3.5901 \n",
            "Epoch 49: loss improved from 3.68010 to 3.65902, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1472 - loss: 3.6124\n",
            "Epoch 50/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1147 - loss: 3.6374\n",
            "Epoch 50: loss improved from 3.65902 to 3.64199, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1153 - loss: 3.6376\n",
            "Epoch 51/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1259 - loss: 3.6261\n",
            "Epoch 51: loss improved from 3.64199 to 3.62018, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1265 - loss: 3.6257\n",
            "Epoch 52/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1433 - loss: 3.5626\n",
            "Epoch 52: loss improved from 3.62018 to 3.59961, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1418 - loss: 3.5667\n",
            "Epoch 53/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1330 - loss: 3.5987\n",
            "Epoch 53: loss improved from 3.59961 to 3.58005, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1330 - loss: 3.5967\n",
            "Epoch 54/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1409 - loss: 3.5755\n",
            "Epoch 54: loss improved from 3.58005 to 3.55994, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1408 - loss: 3.5746\n",
            "Epoch 55/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1446 - loss: 3.5347\n",
            "Epoch 55: loss improved from 3.55994 to 3.54017, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1435 - loss: 3.5353\n",
            "Epoch 56/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1415 - loss: 3.4836\n",
            "Epoch 56: loss improved from 3.54017 to 3.52119, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1416 - loss: 3.4877\n",
            "Epoch 57/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1627 - loss: 3.4306\n",
            "Epoch 57: loss improved from 3.52119 to 3.50242, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1604 - loss: 3.4386\n",
            "Epoch 58/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1404 - loss: 3.4669 \n",
            "Epoch 58: loss improved from 3.50242 to 3.48292, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1389 - loss: 3.4714\n",
            "Epoch 59/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1639 - loss: 3.4809\n",
            "Epoch 59: loss improved from 3.48292 to 3.46347, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1618 - loss: 3.4780\n",
            "Epoch 60/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1686 - loss: 3.4159 \n",
            "Epoch 60: loss improved from 3.46347 to 3.44679, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1628 - loss: 3.4251\n",
            "Epoch 61/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1643 - loss: 3.3417 \n",
            "Epoch 61: loss improved from 3.44679 to 3.42723, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1578 - loss: 3.3647\n",
            "Epoch 62/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1384 - loss: 3.4257 \n",
            "Epoch 62: loss improved from 3.42723 to 3.40892, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1431 - loss: 3.4220\n",
            "Epoch 63/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1528 - loss: 3.3507 \n",
            "Epoch 63: loss improved from 3.40892 to 3.39266, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1535 - loss: 3.3593\n",
            "Epoch 64/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1522 - loss: 3.3509 \n",
            "Epoch 64: loss improved from 3.39266 to 3.37265, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1542 - loss: 3.3571\n",
            "Epoch 65/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1550 - loss: 3.3379 \n",
            "Epoch 65: loss improved from 3.37265 to 3.35871, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1587 - loss: 3.3436\n",
            "Epoch 66/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1767 - loss: 3.3367 \n",
            "Epoch 66: loss improved from 3.35871 to 3.34076, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1755 - loss: 3.3355\n",
            "Epoch 67/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1939 - loss: 3.3145 \n",
            "Epoch 67: loss improved from 3.34076 to 3.32408, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1823 - loss: 3.3192\n",
            "Epoch 68/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1922 - loss: 3.2482 \n",
            "Epoch 68: loss improved from 3.32408 to 3.30572, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1892 - loss: 3.2598\n",
            "Epoch 69/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1572 - loss: 3.2988 \n",
            "Epoch 69: loss improved from 3.30572 to 3.28962, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1597 - loss: 3.2956\n",
            "Epoch 70/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1669 - loss: 3.2847 \n",
            "Epoch 70: loss improved from 3.28962 to 3.27217, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1669 - loss: 3.2825\n",
            "Epoch 71/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2015 - loss: 3.2593 \n",
            "Epoch 71: loss improved from 3.27217 to 3.25513, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1973 - loss: 3.2593\n",
            "Epoch 72/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1837 - loss: 3.1749 \n",
            "Epoch 72: loss improved from 3.25513 to 3.23995, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1804 - loss: 3.1906\n",
            "Epoch 73/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2174 - loss: 3.1471 \n",
            "Epoch 73: loss improved from 3.23995 to 3.22079, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2026 - loss: 3.1785\n",
            "Epoch 74/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1852 - loss: 3.2023  \n",
            "Epoch 74: loss improved from 3.22079 to 3.20466, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1867 - loss: 3.2005\n",
            "Epoch 75/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1863 - loss: 3.1273 \n",
            "Epoch 75: loss improved from 3.20466 to 3.18821, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1885 - loss: 3.1476\n",
            "Epoch 76/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2001 - loss: 3.1376 \n",
            "Epoch 76: loss improved from 3.18821 to 3.17318, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1969 - loss: 3.1480\n",
            "Epoch 77/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2070 - loss: 3.1655 \n",
            "Epoch 77: loss improved from 3.17318 to 3.15430, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2046 - loss: 3.1625\n",
            "Epoch 78/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2134 - loss: 3.1115 \n",
            "Epoch 78: loss improved from 3.15430 to 3.14143, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2102 - loss: 3.1171\n",
            "Epoch 79/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1871 - loss: 3.1302 \n",
            "Epoch 79: loss improved from 3.14143 to 3.12388, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1937 - loss: 3.1289\n",
            "Epoch 80/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2076 - loss: 3.1146  \n",
            "Epoch 80: loss improved from 3.12388 to 3.10789, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2116 - loss: 3.1113\n",
            "Epoch 81/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2333 - loss: 3.1025 \n",
            "Epoch 81: loss improved from 3.10789 to 3.09199, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2280 - loss: 3.0923\n",
            "Epoch 82/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2217 - loss: 3.0653  \n",
            "Epoch 82: loss improved from 3.09199 to 3.07650, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2193 - loss: 3.0680\n",
            "Epoch 83/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1999 - loss: 3.1030 \n",
            "Epoch 83: loss improved from 3.07650 to 3.05966, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2061 - loss: 3.0893\n",
            "Epoch 84/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2750 - loss: 2.9644 \n",
            "Epoch 84: loss improved from 3.05966 to 3.04374, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2581 - loss: 2.9917\n",
            "Epoch 85/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2261 - loss: 3.0338 \n",
            "Epoch 85: loss improved from 3.04374 to 3.02915, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2245 - loss: 3.0337\n",
            "Epoch 86/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2620 - loss: 2.9735 \n",
            "Epoch 86: loss improved from 3.02915 to 3.01273, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2550 - loss: 2.9841\n",
            "Epoch 87/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2235 - loss: 3.0331 \n",
            "Epoch 87: loss improved from 3.01273 to 2.99851, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2301 - loss: 3.0160\n",
            "Epoch 88/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2327 - loss: 2.9180 \n",
            "Epoch 88: loss improved from 2.99851 to 2.98342, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2316 - loss: 2.9324\n",
            "Epoch 89/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2215 - loss: 2.9644 \n",
            "Epoch 89: loss improved from 2.98342 to 2.96812, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2252 - loss: 2.9641\n",
            "Epoch 90/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2597 - loss: 2.9657 \n",
            "Epoch 90: loss improved from 2.96812 to 2.95265, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2547 - loss: 2.9604\n",
            "Epoch 91/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2916 - loss: 2.8870 \n",
            "Epoch 91: loss improved from 2.95265 to 2.93829, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2777 - loss: 2.9031\n",
            "Epoch 92/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2665 - loss: 2.8878 \n",
            "Epoch 92: loss improved from 2.93829 to 2.92240, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2640 - loss: 2.8974\n",
            "Epoch 93/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2657 - loss: 2.9298 \n",
            "Epoch 93: loss improved from 2.92240 to 2.90811, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2620 - loss: 2.9248\n",
            "Epoch 94/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2857 - loss: 2.8539 \n",
            "Epoch 94: loss improved from 2.90811 to 2.89521, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2788 - loss: 2.8707\n",
            "Epoch 95/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2831 - loss: 2.8495  \n",
            "Epoch 95: loss improved from 2.89521 to 2.87898, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2810 - loss: 2.8584\n",
            "Epoch 96/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3049 - loss: 2.7463 \n",
            "Epoch 96: loss improved from 2.87898 to 2.86519, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2927 - loss: 2.7815\n",
            "Epoch 97/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2768 - loss: 2.8888 \n",
            "Epoch 97: loss improved from 2.86519 to 2.85219, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2734 - loss: 2.8771\n",
            "Epoch 98/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2877 - loss: 2.7892 \n",
            "Epoch 98: loss improved from 2.85219 to 2.83817, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2841 - loss: 2.8009\n",
            "Epoch 99/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2812 - loss: 2.8015 \n",
            "Epoch 99: loss improved from 2.83817 to 2.82512, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2790 - loss: 2.8074\n",
            "Epoch 100/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3125 - loss: 2.7121 \n",
            "Epoch 100: loss improved from 2.82512 to 2.80801, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3021 - loss: 2.7369\n",
            "Epoch 101/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3153 - loss: 2.7751  \n",
            "Epoch 101: loss improved from 2.80801 to 2.79584, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3101 - loss: 2.7795\n",
            "Epoch 102/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2798 - loss: 2.8071 \n",
            "Epoch 102: loss improved from 2.79584 to 2.78366, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2843 - loss: 2.7972\n",
            "Epoch 103/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3362 - loss: 2.7406 \n",
            "Epoch 103: loss improved from 2.78366 to 2.76853, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3200 - loss: 2.7524\n",
            "Epoch 104/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3209 - loss: 2.7211 \n",
            "Epoch 104: loss improved from 2.76853 to 2.75428, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3101 - loss: 2.7382\n",
            "Epoch 105/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3359 - loss: 2.6851 \n",
            "Epoch 105: loss improved from 2.75428 to 2.74297, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3287 - loss: 2.6986\n",
            "Epoch 106/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3018 - loss: 2.6818 \n",
            "Epoch 106: loss improved from 2.74297 to 2.72798, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3017 - loss: 2.6937\n",
            "Epoch 107/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2944 - loss: 2.7259 \n",
            "Epoch 107: loss improved from 2.72798 to 2.71730, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2955 - loss: 2.7243\n",
            "Epoch 108/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3360 - loss: 2.6857 \n",
            "Epoch 108: loss improved from 2.71730 to 2.70571, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3292 - loss: 2.6878\n",
            "Epoch 109/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2996 - loss: 2.6939 \n",
            "Epoch 109: loss improved from 2.70571 to 2.69171, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2986 - loss: 2.6942\n",
            "Epoch 110/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3349 - loss: 2.6935 \n",
            "Epoch 110: loss improved from 2.69171 to 2.67531, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3232 - loss: 2.6880\n",
            "Epoch 111/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3420 - loss: 2.6425 \n",
            "Epoch 111: loss improved from 2.67531 to 2.66357, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3273 - loss: 2.6563\n",
            "Epoch 112/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2840 - loss: 2.6695 \n",
            "Epoch 112: loss improved from 2.66357 to 2.65170, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2915 - loss: 2.6626\n",
            "Epoch 113/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3264 - loss: 2.6359 \n",
            "Epoch 113: loss improved from 2.65170 to 2.63952, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3250 - loss: 2.6354\n",
            "Epoch 114/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3021 - loss: 2.6185 \n",
            "Epoch 114: loss improved from 2.63952 to 2.63094, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3061 - loss: 2.6221\n",
            "Epoch 115/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3050 - loss: 2.6201 \n",
            "Epoch 115: loss improved from 2.63094 to 2.61236, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3094 - loss: 2.6205\n",
            "Epoch 116/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3458 - loss: 2.5902 \n",
            "Epoch 116: loss improved from 2.61236 to 2.60240, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3396 - loss: 2.5940\n",
            "Epoch 117/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3437 - loss: 2.5970 \n",
            "Epoch 117: loss improved from 2.60240 to 2.59044, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3405 - loss: 2.5924\n",
            "Epoch 118/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3592 - loss: 2.5412 \n",
            "Epoch 118: loss improved from 2.59044 to 2.57706, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3535 - loss: 2.5561\n",
            "Epoch 119/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3748 - loss: 2.5548  \n",
            "Epoch 119: loss improved from 2.57706 to 2.56624, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3626 - loss: 2.5575\n",
            "Epoch 120/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3635 - loss: 2.4850 \n",
            "Epoch 120: loss improved from 2.56624 to 2.55398, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3497 - loss: 2.5206\n",
            "Epoch 121/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3559 - loss: 2.4950 \n",
            "Epoch 121: loss improved from 2.55398 to 2.54212, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3480 - loss: 2.5124\n",
            "Epoch 122/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3503 - loss: 2.5294 \n",
            "Epoch 122: loss improved from 2.54212 to 2.53029, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3481 - loss: 2.5293\n",
            "Epoch 123/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4009 - loss: 2.4784 \n",
            "Epoch 123: loss improved from 2.53029 to 2.51845, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3797 - loss: 2.4935\n",
            "Epoch 124/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3746 - loss: 2.4736 \n",
            "Epoch 124: loss improved from 2.51845 to 2.50924, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3630 - loss: 2.4950\n",
            "Epoch 125/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3503 - loss: 2.4828\n",
            "Epoch 125: loss improved from 2.50924 to 2.49806, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3502 - loss: 2.4836\n",
            "Epoch 126/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3624 - loss: 2.4608 \n",
            "Epoch 126: loss improved from 2.49806 to 2.48510, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3576 - loss: 2.4627\n",
            "Epoch 127/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3581 - loss: 2.5294 \n",
            "Epoch 127: loss improved from 2.48510 to 2.47504, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3546 - loss: 2.5057\n",
            "Epoch 128/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3587 - loss: 2.4547 \n",
            "Epoch 128: loss improved from 2.47504 to 2.46162, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3583 - loss: 2.4558\n",
            "Epoch 129/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4065 - loss: 2.3739\n",
            "Epoch 129: loss improved from 2.46162 to 2.45145, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4020 - loss: 2.3825\n",
            "Epoch 130/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3700 - loss: 2.4124\n",
            "Epoch 130: loss improved from 2.45145 to 2.43817, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3690 - loss: 2.4139\n",
            "Epoch 131/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3647 - loss: 2.3957 \n",
            "Epoch 131: loss improved from 2.43817 to 2.42566, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3680 - loss: 2.4025\n",
            "Epoch 132/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3822 - loss: 2.3915\n",
            "Epoch 132: loss improved from 2.42566 to 2.41593, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3797 - loss: 2.3955\n",
            "Epoch 133/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3891 - loss: 2.3769\n",
            "Epoch 133: loss improved from 2.41593 to 2.40732, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3879 - loss: 2.3786\n",
            "Epoch 134/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3581 - loss: 2.3850\n",
            "Epoch 134: loss improved from 2.40732 to 2.39501, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3590 - loss: 2.3855\n",
            "Epoch 135/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4127 - loss: 2.3448\n",
            "Epoch 135: loss improved from 2.39501 to 2.38454, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4091 - loss: 2.3492\n",
            "Epoch 136/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3929 - loss: 2.3947 \n",
            "Epoch 136: loss improved from 2.38454 to 2.37089, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3868 - loss: 2.3783\n",
            "Epoch 137/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3634 - loss: 2.3796 \n",
            "Epoch 137: loss improved from 2.37089 to 2.36278, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3719 - loss: 2.3716\n",
            "Epoch 138/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4165 - loss: 2.3006  \n",
            "Epoch 138: loss improved from 2.36278 to 2.35186, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4103 - loss: 2.3138\n",
            "Epoch 139/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3873 - loss: 2.2963 \n",
            "Epoch 139: loss improved from 2.35186 to 2.34488, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3857 - loss: 2.3117\n",
            "Epoch 140/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4202 - loss: 2.2822 \n",
            "Epoch 140: loss improved from 2.34488 to 2.33352, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4077 - loss: 2.2985\n",
            "Epoch 141/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3849 - loss: 2.3580 \n",
            "Epoch 141: loss improved from 2.33352 to 2.32548, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3912 - loss: 2.3440\n",
            "Epoch 142/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4082 - loss: 2.2947 \n",
            "Epoch 142: loss improved from 2.32548 to 2.31280, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4056 - loss: 2.3010\n",
            "Epoch 143/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4291 - loss: 2.2596 \n",
            "Epoch 143: loss improved from 2.31280 to 2.30194, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4222 - loss: 2.2726\n",
            "Epoch 144/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4265 - loss: 2.2809 \n",
            "Epoch 144: loss improved from 2.30194 to 2.29104, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4211 - loss: 2.2815\n",
            "Epoch 145/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4129 - loss: 2.3113  \n",
            "Epoch 145: loss improved from 2.29104 to 2.28079, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4099 - loss: 2.3021\n",
            "Epoch 146/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4317 - loss: 2.2818 \n",
            "Epoch 146: loss improved from 2.28079 to 2.27017, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4298 - loss: 2.2774\n",
            "Epoch 147/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4129 - loss: 2.2589 \n",
            "Epoch 147: loss improved from 2.27017 to 2.26356, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4135 - loss: 2.2588\n",
            "Epoch 148/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4366 - loss: 2.2667 \n",
            "Epoch 148: loss improved from 2.26356 to 2.25229, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4289 - loss: 2.2579\n",
            "Epoch 149/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4288 - loss: 2.2138  \n",
            "Epoch 149: loss improved from 2.25229 to 2.24242, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4242 - loss: 2.2244\n",
            "Epoch 150/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4418 - loss: 2.2103 \n",
            "Epoch 150: loss improved from 2.24242 to 2.23594, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4370 - loss: 2.2170\n",
            "Epoch 151/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4382 - loss: 2.1776 \n",
            "Epoch 151: loss improved from 2.23594 to 2.22317, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4354 - loss: 2.1877\n",
            "Epoch 152/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4523 - loss: 2.1399 \n",
            "Epoch 152: loss improved from 2.22317 to 2.21323, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4431 - loss: 2.1600\n",
            "Epoch 153/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4637 - loss: 2.1392 \n",
            "Epoch 153: loss improved from 2.21323 to 2.20500, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4532 - loss: 2.1570\n",
            "Epoch 154/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4529 - loss: 2.1724 \n",
            "Epoch 154: loss improved from 2.20500 to 2.19765, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4460 - loss: 2.1814\n",
            "Epoch 155/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4684 - loss: 2.1175 \n",
            "Epoch 155: loss improved from 2.19765 to 2.18530, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4571 - loss: 2.1398\n",
            "Epoch 156/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4562 - loss: 2.1706  \n",
            "Epoch 156: loss improved from 2.18530 to 2.17625, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4526 - loss: 2.1684\n",
            "Epoch 157/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4713 - loss: 2.0860 \n",
            "Epoch 157: loss improved from 2.17625 to 2.16935, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4629 - loss: 2.1070\n",
            "Epoch 158/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4311 - loss: 2.1770 \n",
            "Epoch 158: loss improved from 2.16935 to 2.15822, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4350 - loss: 2.1716\n",
            "Epoch 159/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4418 - loss: 2.1365 \n",
            "Epoch 159: loss improved from 2.15822 to 2.14825, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4434 - loss: 2.1354\n",
            "Epoch 160/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4323 - loss: 2.1512 \n",
            "Epoch 160: loss improved from 2.14825 to 2.14143, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4354 - loss: 2.1449\n",
            "Epoch 161/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4405 - loss: 2.1064 \n",
            "Epoch 161: loss improved from 2.14143 to 2.13054, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4418 - loss: 2.1119\n",
            "Epoch 162/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4445 - loss: 2.1691 \n",
            "Epoch 162: loss improved from 2.13054 to 2.12210, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4504 - loss: 2.1500\n",
            "Epoch 163/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4616 - loss: 2.0845  \n",
            "Epoch 163: loss improved from 2.12210 to 2.11447, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4566 - loss: 2.0931\n",
            "Epoch 164/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4794 - loss: 2.0830 \n",
            "Epoch 164: loss improved from 2.11447 to 2.10526, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4732 - loss: 2.0918\n",
            "Epoch 165/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4727 - loss: 2.0723 \n",
            "Epoch 165: loss improved from 2.10526 to 2.09725, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4670 - loss: 2.0812\n",
            "Epoch 166/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4325 - loss: 2.0899 \n",
            "Epoch 166: loss improved from 2.09725 to 2.08863, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4373 - loss: 2.0879\n",
            "Epoch 167/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4916 - loss: 2.0758  \n",
            "Epoch 167: loss improved from 2.08863 to 2.08070, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4823 - loss: 2.0762\n",
            "Epoch 168/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4645 - loss: 2.0931 \n",
            "Epoch 168: loss improved from 2.08070 to 2.07028, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4679 - loss: 2.0830\n",
            "Epoch 169/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4775 - loss: 2.0951 \n",
            "Epoch 169: loss improved from 2.07028 to 2.06500, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4747 - loss: 2.0876\n",
            "Epoch 170/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4796 - loss: 2.0389 \n",
            "Epoch 170: loss improved from 2.06500 to 2.05579, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4755 - loss: 2.0433\n",
            "Epoch 171/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4970 - loss: 1.9621 \n",
            "Epoch 171: loss improved from 2.05579 to 2.04496, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4818 - loss: 1.9913\n",
            "Epoch 172/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4780 - loss: 2.0564 \n",
            "Epoch 172: loss improved from 2.04496 to 2.03926, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4762 - loss: 2.0515\n",
            "Epoch 173/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5208 - loss: 1.9781  \n",
            "Epoch 173: loss improved from 2.03926 to 2.03047, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5040 - loss: 2.0003\n",
            "Epoch 174/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4658 - loss: 2.0376  \n",
            "Epoch 174: loss improved from 2.03047 to 2.02161, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4680 - loss: 2.0307\n",
            "Epoch 175/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4682 - loss: 2.0119 \n",
            "Epoch 175: loss improved from 2.02161 to 2.01337, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4712 - loss: 2.0120\n",
            "Epoch 176/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4749 - loss: 1.9772 \n",
            "Epoch 176: loss improved from 2.01337 to 2.00726, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4767 - loss: 1.9830\n",
            "Epoch 177/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4892 - loss: 1.9597 \n",
            "Epoch 177: loss improved from 2.00726 to 2.00011, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4875 - loss: 1.9675\n",
            "Epoch 178/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4874 - loss: 2.0078 \n",
            "Epoch 178: loss improved from 2.00011 to 1.99181, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4873 - loss: 2.0036\n",
            "Epoch 179/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5100 - loss: 2.0020 \n",
            "Epoch 179: loss improved from 1.99181 to 1.98095, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5042 - loss: 1.9969\n",
            "Epoch 180/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4648 - loss: 1.9998  \n",
            "Epoch 180: loss improved from 1.98095 to 1.97524, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4692 - loss: 1.9905\n",
            "Epoch 181/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4928 - loss: 1.9752 \n",
            "Epoch 181: loss improved from 1.97524 to 1.96706, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4918 - loss: 1.9712\n",
            "Epoch 182/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5071 - loss: 1.9191 \n",
            "Epoch 182: loss improved from 1.96706 to 1.96240, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5016 - loss: 1.9289\n",
            "Epoch 183/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4887 - loss: 1.9486 \n",
            "Epoch 183: loss improved from 1.96240 to 1.95244, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4904 - loss: 1.9469\n",
            "Epoch 184/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5061 - loss: 1.9593 \n",
            "Epoch 184: loss improved from 1.95244 to 1.94516, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5025 - loss: 1.9519\n",
            "Epoch 185/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5420 - loss: 1.8653  \n",
            "Epoch 185: loss improved from 1.94516 to 1.93709, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5323 - loss: 1.8822\n",
            "Epoch 186/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5236 - loss: 1.8664 \n",
            "Epoch 186: loss improved from 1.93709 to 1.92949, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5162 - loss: 1.8831\n",
            "Epoch 187/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5529 - loss: 1.8682 \n",
            "Epoch 187: loss improved from 1.92949 to 1.92446, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5407 - loss: 1.8820\n",
            "Epoch 188/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4862 - loss: 1.9014 \n",
            "Epoch 188: loss improved from 1.92446 to 1.91574, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4882 - loss: 1.9044\n",
            "Epoch 189/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5537 - loss: 1.8574 \n",
            "Epoch 189: loss improved from 1.91574 to 1.91000, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5373 - loss: 1.8755\n",
            "Epoch 190/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4965 - loss: 1.9528 \n",
            "Epoch 190: loss improved from 1.91000 to 1.90060, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4996 - loss: 1.9363\n",
            "Epoch 191/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5087 - loss: 1.8728 \n",
            "Epoch 191: loss improved from 1.90060 to 1.89403, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5072 - loss: 1.8834\n",
            "Epoch 192/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5131 - loss: 1.9117  \n",
            "Epoch 192: loss improved from 1.89403 to 1.88678, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5123 - loss: 1.9036\n",
            "Epoch 193/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5504 - loss: 1.8201 \n",
            "Epoch 193: loss improved from 1.88678 to 1.87721, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5406 - loss: 1.8349\n",
            "Epoch 194/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5174 - loss: 1.8779 \n",
            "Epoch 194: loss improved from 1.87721 to 1.87373, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5155 - loss: 1.8744\n",
            "Epoch 195/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4972 - loss: 1.8535 \n",
            "Epoch 195: loss improved from 1.87373 to 1.86570, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4997 - loss: 1.8577\n",
            "Epoch 196/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5190 - loss: 1.8456\n",
            "Epoch 196: loss improved from 1.86570 to 1.85850, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5186 - loss: 1.8463\n",
            "Epoch 197/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5352 - loss: 1.7984\n",
            "Epoch 197: loss improved from 1.85850 to 1.85094, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5344 - loss: 1.8042\n",
            "Epoch 198/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5561 - loss: 1.8408\n",
            "Epoch 198: loss improved from 1.85094 to 1.84699, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5507 - loss: 1.8416\n",
            "Epoch 199/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5569 - loss: 1.7714  \n",
            "Epoch 199: loss improved from 1.84699 to 1.83851, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5381 - loss: 1.8003\n",
            "Epoch 200/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5335 - loss: 1.8344 \n",
            "Epoch 200: loss improved from 1.83851 to 1.83098, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5322 - loss: 1.8343\n",
            "Epoch 201/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5274 - loss: 1.8279\n",
            "Epoch 201: loss improved from 1.83098 to 1.82538, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5275 - loss: 1.8276\n",
            "Epoch 202/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5756 - loss: 1.7376\n",
            "Epoch 202: loss improved from 1.82538 to 1.81735, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5640 - loss: 1.7584\n",
            "Epoch 203/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5317 - loss: 1.7700\n",
            "Epoch 203: loss improved from 1.81735 to 1.81108, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5301 - loss: 1.7746\n",
            "Epoch 204/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5233 - loss: 1.8075\n",
            "Epoch 204: loss improved from 1.81108 to 1.80463, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5255 - loss: 1.8072\n",
            "Epoch 205/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5169 - loss: 1.7876\n",
            "Epoch 205: loss improved from 1.80463 to 1.79661, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5167 - loss: 1.7886\n",
            "Epoch 206/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5078 - loss: 1.8335 \n",
            "Epoch 206: loss improved from 1.79661 to 1.79403, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5161 - loss: 1.8219\n",
            "Epoch 207/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5415 - loss: 1.7027  \n",
            "Epoch 207: loss improved from 1.79403 to 1.78690, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5350 - loss: 1.7334\n",
            "Epoch 208/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5142 - loss: 1.7670 \n",
            "Epoch 208: loss improved from 1.78690 to 1.78086, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5202 - loss: 1.7691\n",
            "Epoch 209/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5377 - loss: 1.7879 \n",
            "Epoch 209: loss improved from 1.78086 to 1.77516, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5356 - loss: 1.7855\n",
            "Epoch 210/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5496 - loss: 1.7854 \n",
            "Epoch 210: loss improved from 1.77516 to 1.76300, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5471 - loss: 1.7783\n",
            "Epoch 211/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5536 - loss: 1.7924 \n",
            "Epoch 211: loss improved from 1.76300 to 1.75787, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5511 - loss: 1.7849\n",
            "Epoch 212/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5273 - loss: 1.7618 \n",
            "Epoch 212: loss improved from 1.75787 to 1.75496, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5326 - loss: 1.7535\n",
            "Epoch 213/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5587 - loss: 1.7059  \n",
            "Epoch 213: loss improved from 1.75496 to 1.75020, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5519 - loss: 1.7176\n",
            "Epoch 214/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5883 - loss: 1.6808 \n",
            "Epoch 214: loss improved from 1.75020 to 1.74196, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5721 - loss: 1.6988\n",
            "Epoch 215/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5638 - loss: 1.7354  \n",
            "Epoch 215: loss improved from 1.74196 to 1.73687, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5597 - loss: 1.7343\n",
            "Epoch 216/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5441 - loss: 1.7288 \n",
            "Epoch 216: loss improved from 1.73687 to 1.72768, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5470 - loss: 1.7257\n",
            "Epoch 217/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5750 - loss: 1.7179 \n",
            "Epoch 217: loss improved from 1.72768 to 1.72238, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5715 - loss: 1.7164\n",
            "Epoch 218/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5704 - loss: 1.6830 \n",
            "Epoch 218: loss improved from 1.72238 to 1.71695, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5649 - loss: 1.6931\n",
            "Epoch 219/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5509 - loss: 1.6604 \n",
            "Epoch 219: loss improved from 1.71695 to 1.71188, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5480 - loss: 1.6845\n",
            "Epoch 220/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5646 - loss: 1.6977  \n",
            "Epoch 220: loss improved from 1.71188 to 1.70582, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5614 - loss: 1.6981\n",
            "Epoch 221/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5820 - loss: 1.6757 \n",
            "Epoch 221: loss improved from 1.70582 to 1.70251, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5771 - loss: 1.6821\n",
            "Epoch 222/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5961 - loss: 1.6356 \n",
            "Epoch 222: loss improved from 1.70251 to 1.69509, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5730 - loss: 1.6711\n",
            "Epoch 223/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5453 - loss: 1.7102 \n",
            "Epoch 223: loss improved from 1.69509 to 1.69022, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5465 - loss: 1.7045\n",
            "Epoch 224/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5350 - loss: 1.7183 \n",
            "Epoch 224: loss improved from 1.69022 to 1.67986, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5398 - loss: 1.7104\n",
            "Epoch 225/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5718 - loss: 1.6671 \n",
            "Epoch 225: loss improved from 1.67986 to 1.67650, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5675 - loss: 1.6706\n",
            "Epoch 226/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5507 - loss: 1.7083  \n",
            "Epoch 226: loss improved from 1.67650 to 1.67041, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5560 - loss: 1.6952\n",
            "Epoch 227/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5615 - loss: 1.6897 \n",
            "Epoch 227: loss improved from 1.67041 to 1.66355, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5634 - loss: 1.6828\n",
            "Epoch 228/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5613 - loss: 1.6855 \n",
            "Epoch 228: loss improved from 1.66355 to 1.65972, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5650 - loss: 1.6752\n",
            "Epoch 229/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5800 - loss: 1.6534 \n",
            "Epoch 229: loss improved from 1.65972 to 1.65534, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5796 - loss: 1.6511\n",
            "Epoch 230/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5542 - loss: 1.6994 \n",
            "Epoch 230: loss improved from 1.65534 to 1.64802, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5594 - loss: 1.6810\n",
            "Epoch 231/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5568 - loss: 1.6765 \n",
            "Epoch 231: loss improved from 1.64802 to 1.64140, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5632 - loss: 1.6618\n",
            "Epoch 232/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5922 - loss: 1.6351 \n",
            "Epoch 232: loss improved from 1.64140 to 1.63638, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5866 - loss: 1.6350\n",
            "Epoch 233/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6181 - loss: 1.5699  \n",
            "Epoch 233: loss improved from 1.63638 to 1.63082, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6066 - loss: 1.5861\n",
            "Epoch 234/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5385 - loss: 1.6249 \n",
            "Epoch 234: loss improved from 1.63082 to 1.62619, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5449 - loss: 1.6253\n",
            "Epoch 235/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6079 - loss: 1.5974 \n",
            "Epoch 235: loss improved from 1.62619 to 1.62088, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6006 - loss: 1.6037\n",
            "Epoch 236/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5723 - loss: 1.6323 \n",
            "Epoch 236: loss improved from 1.62088 to 1.61364, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5730 - loss: 1.6258\n",
            "Epoch 237/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5511 - loss: 1.6620  \n",
            "Epoch 237: loss improved from 1.61364 to 1.60985, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5545 - loss: 1.6512\n",
            "Epoch 238/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6047 - loss: 1.5914 \n",
            "Epoch 238: loss improved from 1.60985 to 1.60622, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5967 - loss: 1.5981\n",
            "Epoch 239/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6143 - loss: 1.5493 \n",
            "Epoch 239: loss improved from 1.60622 to 1.60063, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6071 - loss: 1.5612\n",
            "Epoch 240/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5959 - loss: 1.5716 \n",
            "Epoch 240: loss improved from 1.60063 to 1.59324, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5926 - loss: 1.5766\n",
            "Epoch 241/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5892 - loss: 1.6001 \n",
            "Epoch 241: loss improved from 1.59324 to 1.59073, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5882 - loss: 1.5958\n",
            "Epoch 242/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5988 - loss: 1.5789 \n",
            "Epoch 242: loss improved from 1.59073 to 1.58373, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5970 - loss: 1.5798\n",
            "Epoch 243/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5561 - loss: 1.6590 \n",
            "Epoch 243: loss improved from 1.58373 to 1.58054, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5661 - loss: 1.6267\n",
            "Epoch 244/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5853 - loss: 1.5695 \n",
            "Epoch 244: loss improved from 1.58054 to 1.57542, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5826 - loss: 1.5726\n",
            "Epoch 245/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5882 - loss: 1.5702 \n",
            "Epoch 245: loss improved from 1.57542 to 1.56991, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5898 - loss: 1.5647\n",
            "Epoch 246/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5967 - loss: 1.5609 \n",
            "Epoch 246: loss improved from 1.56991 to 1.56302, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5952 - loss: 1.5592\n",
            "Epoch 247/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6065 - loss: 1.5303 \n",
            "Epoch 247: loss improved from 1.56302 to 1.55742, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6034 - loss: 1.5364\n",
            "Epoch 248/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6385 - loss: 1.5156 \n",
            "Epoch 248: loss improved from 1.55742 to 1.55379, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6277 - loss: 1.5262\n",
            "Epoch 249/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6120 - loss: 1.5546 \n",
            "Epoch 249: loss improved from 1.55379 to 1.54679, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6053 - loss: 1.5533\n",
            "Epoch 250/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6141 - loss: 1.4721  \n",
            "Epoch 250: loss improved from 1.54679 to 1.54499, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6070 - loss: 1.4959\n",
            "Epoch 251/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5900 - loss: 1.5642  \n",
            "Epoch 251: loss improved from 1.54499 to 1.53725, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5937 - loss: 1.5584\n",
            "Epoch 252/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6372 - loss: 1.4291 \n",
            "Epoch 252: loss improved from 1.53725 to 1.53421, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6251 - loss: 1.4627\n",
            "Epoch 253/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6039 - loss: 1.5328 \n",
            "Epoch 253: loss improved from 1.53421 to 1.52608, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6048 - loss: 1.5319\n",
            "Epoch 254/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6510 - loss: 1.4809 \n",
            "Epoch 254: loss improved from 1.52608 to 1.52209, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6409 - loss: 1.4911\n",
            "Epoch 255/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5844 - loss: 1.5759 \n",
            "Epoch 255: loss improved from 1.52209 to 1.51537, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5974 - loss: 1.5518\n",
            "Epoch 256/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6289 - loss: 1.4595 \n",
            "Epoch 256: loss improved from 1.51537 to 1.51184, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6207 - loss: 1.4754\n",
            "Epoch 257/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6048 - loss: 1.4886 \n",
            "Epoch 257: loss improved from 1.51184 to 1.50841, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6111 - loss: 1.4913\n",
            "Epoch 258/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6494 - loss: 1.4684  \n",
            "Epoch 258: loss improved from 1.50841 to 1.50349, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6415 - loss: 1.4775\n",
            "Epoch 259/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6420 - loss: 1.4805 \n",
            "Epoch 259: loss improved from 1.50349 to 1.49658, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6344 - loss: 1.4865\n",
            "Epoch 260/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6119 - loss: 1.5363 \n",
            "Epoch 260: loss improved from 1.49658 to 1.49293, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6140 - loss: 1.5202\n",
            "Epoch 261/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5928 - loss: 1.5431 \n",
            "Epoch 261: loss improved from 1.49293 to 1.49117, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5993 - loss: 1.5207\n",
            "Epoch 262/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6380 - loss: 1.4753 \n",
            "Epoch 262: loss improved from 1.49117 to 1.48415, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6282 - loss: 1.4812\n",
            "Epoch 263/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6574 - loss: 1.4750 \n",
            "Epoch 263: loss improved from 1.48415 to 1.47854, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6462 - loss: 1.4759\n",
            "Epoch 264/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6111 - loss: 1.4825  \n",
            "Epoch 264: loss improved from 1.47854 to 1.47428, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6148 - loss: 1.4799\n",
            "Epoch 265/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6152 - loss: 1.4894 \n",
            "Epoch 265: loss improved from 1.47428 to 1.46848, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6171 - loss: 1.4792\n",
            "Epoch 266/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6466 - loss: 1.4710  \n",
            "Epoch 266: loss improved from 1.46848 to 1.46362, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6394 - loss: 1.4670\n",
            "Epoch 267/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6349 - loss: 1.4834 \n",
            "Epoch 267: loss improved from 1.46362 to 1.45955, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6345 - loss: 1.4792\n",
            "Epoch 268/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6261 - loss: 1.4368  \n",
            "Epoch 268: loss improved from 1.45955 to 1.45460, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6322 - loss: 1.4335\n",
            "Epoch 269/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6282 - loss: 1.4425\n",
            "Epoch 269: loss improved from 1.45460 to 1.45233, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6287 - loss: 1.4442\n",
            "Epoch 270/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6269 - loss: 1.4292\n",
            "Epoch 270: loss improved from 1.45233 to 1.44836, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6277 - loss: 1.4325\n",
            "Epoch 271/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6434 - loss: 1.3896 \n",
            "Epoch 271: loss improved from 1.44836 to 1.44178, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6396 - loss: 1.4084\n",
            "Epoch 272/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6717 - loss: 1.3624\n",
            "Epoch 272: loss did not improve from 1.44178\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6697 - loss: 1.3669\n",
            "Epoch 273/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6750 - loss: 1.4045\n",
            "Epoch 273: loss improved from 1.44178 to 1.43403, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6730 - loss: 1.4061\n",
            "Epoch 274/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6420 - loss: 1.4233\n",
            "Epoch 274: loss improved from 1.43403 to 1.43043, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6406 - loss: 1.4249\n",
            "Epoch 275/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6204 - loss: 1.4853 \n",
            "Epoch 275: loss improved from 1.43043 to 1.42454, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6230 - loss: 1.4698\n",
            "Epoch 276/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6429 - loss: 1.3887\n",
            "Epoch 276: loss improved from 1.42454 to 1.41957, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6391 - loss: 1.4003\n",
            "Epoch 277/400\n",
            "\u001b[1m 8/17\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6565 - loss: 1.4359  \n",
            "Epoch 277: loss improved from 1.41957 to 1.41449, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6513 - loss: 1.4253\n",
            "Epoch 278/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6550 - loss: 1.4114 \n",
            "Epoch 278: loss improved from 1.41449 to 1.41043, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6510 - loss: 1.4117\n",
            "Epoch 279/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6515 - loss: 1.4118  \n",
            "Epoch 279: loss improved from 1.41043 to 1.40620, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6498 - loss: 1.4129\n",
            "Epoch 280/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6282 - loss: 1.4141 \n",
            "Epoch 280: loss improved from 1.40620 to 1.40325, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6324 - loss: 1.4109\n",
            "Epoch 281/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6524 - loss: 1.3908 \n",
            "Epoch 281: loss improved from 1.40325 to 1.39982, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6492 - loss: 1.3911\n",
            "Epoch 282/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6979 - loss: 1.3608 \n",
            "Epoch 282: loss improved from 1.39982 to 1.39387, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6786 - loss: 1.3831\n",
            "Epoch 283/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6611 - loss: 1.3532  \n",
            "Epoch 283: loss improved from 1.39387 to 1.38889, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6579 - loss: 1.3659\n",
            "Epoch 284/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6837 - loss: 1.3381  \n",
            "Epoch 284: loss improved from 1.38889 to 1.38598, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6781 - loss: 1.3508\n",
            "Epoch 285/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6559 - loss: 1.3432 \n",
            "Epoch 285: loss improved from 1.38598 to 1.38267, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6557 - loss: 1.3551\n",
            "Epoch 286/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6509 - loss: 1.4018 \n",
            "Epoch 286: loss improved from 1.38267 to 1.37652, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6531 - loss: 1.3928\n",
            "Epoch 287/400\n",
            "\u001b[1m 8/17\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6348 - loss: 1.4482 \n",
            "Epoch 287: loss improved from 1.37652 to 1.37329, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6441 - loss: 1.4111\n",
            "Epoch 288/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6670 - loss: 1.3628  \n",
            "Epoch 288: loss improved from 1.37329 to 1.36718, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6657 - loss: 1.3626\n",
            "Epoch 289/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6519 - loss: 1.3979  \n",
            "Epoch 289: loss improved from 1.36718 to 1.36374, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6524 - loss: 1.3883\n",
            "Epoch 290/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6864 - loss: 1.3165 \n",
            "Epoch 290: loss improved from 1.36374 to 1.36061, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6785 - loss: 1.3280\n",
            "Epoch 291/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6691 - loss: 1.3311 \n",
            "Epoch 291: loss improved from 1.36061 to 1.35578, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6685 - loss: 1.3361\n",
            "Epoch 292/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6911 - loss: 1.2758 \n",
            "Epoch 292: loss improved from 1.35578 to 1.35276, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6815 - loss: 1.3077\n",
            "Epoch 293/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7143 - loss: 1.2489 \n",
            "Epoch 293: loss improved from 1.35276 to 1.34782, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7017 - loss: 1.2762\n",
            "Epoch 294/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7030 - loss: 1.3132 \n",
            "Epoch 294: loss improved from 1.34782 to 1.34294, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6895 - loss: 1.3275\n",
            "Epoch 295/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6327 - loss: 1.3895 \n",
            "Epoch 295: loss improved from 1.34294 to 1.34285, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6462 - loss: 1.3715\n",
            "Epoch 296/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6642 - loss: 1.3529  \n",
            "Epoch 296: loss improved from 1.34285 to 1.34088, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6635 - loss: 1.3493\n",
            "Epoch 297/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6771 - loss: 1.3390 \n",
            "Epoch 297: loss improved from 1.34088 to 1.33246, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6754 - loss: 1.3334\n",
            "Epoch 298/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6796 - loss: 1.3141 \n",
            "Epoch 298: loss did not improve from 1.33246\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6726 - loss: 1.3235\n",
            "Epoch 299/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6745 - loss: 1.3574 \n",
            "Epoch 299: loss improved from 1.33246 to 1.32474, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6779 - loss: 1.3462\n",
            "Epoch 300/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6733 - loss: 1.3297 \n",
            "Epoch 300: loss improved from 1.32474 to 1.32202, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6721 - loss: 1.3306\n",
            "Epoch 301/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6902 - loss: 1.3096  \n",
            "Epoch 301: loss improved from 1.32202 to 1.31508, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6859 - loss: 1.3155\n",
            "Epoch 302/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6800 - loss: 1.2783 \n",
            "Epoch 302: loss improved from 1.31508 to 1.31368, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6778 - loss: 1.2912\n",
            "Epoch 303/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7177 - loss: 1.2113  \n",
            "Epoch 303: loss improved from 1.31368 to 1.30933, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6980 - loss: 1.2489\n",
            "Epoch 304/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6760 - loss: 1.3329  \n",
            "Epoch 304: loss improved from 1.30933 to 1.30456, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6757 - loss: 1.3253\n",
            "Epoch 305/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6808 - loss: 1.3399 \n",
            "Epoch 305: loss improved from 1.30456 to 1.30055, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6780 - loss: 1.3256\n",
            "Epoch 306/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6738 - loss: 1.3484 \n",
            "Epoch 306: loss improved from 1.30055 to 1.29837, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6740 - loss: 1.3373\n",
            "Epoch 307/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7028 - loss: 1.2470  \n",
            "Epoch 307: loss improved from 1.29837 to 1.29241, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6982 - loss: 1.2581\n",
            "Epoch 308/400\n",
            "\u001b[1m 6/17\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7118 - loss: 1.2221\n",
            "Epoch 308: loss improved from 1.29241 to 1.29110, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7013 - loss: 1.2495 \n",
            "Epoch 309/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6823 - loss: 1.2942  \n",
            "Epoch 309: loss improved from 1.29110 to 1.28569, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6867 - loss: 1.2872\n",
            "Epoch 310/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7148 - loss: 1.2180 \n",
            "Epoch 310: loss improved from 1.28569 to 1.28341, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7055 - loss: 1.2363\n",
            "Epoch 311/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6955 - loss: 1.2634  \n",
            "Epoch 311: loss improved from 1.28341 to 1.27935, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6904 - loss: 1.2705\n",
            "Epoch 312/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6889 - loss: 1.2922 \n",
            "Epoch 312: loss improved from 1.27935 to 1.27507, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6891 - loss: 1.2890\n",
            "Epoch 313/400\n",
            "\u001b[1m 8/17\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7081 - loss: 1.2057 \n",
            "Epoch 313: loss improved from 1.27507 to 1.26957, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7007 - loss: 1.2289\n",
            "Epoch 314/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 1.2647 \n",
            "Epoch 314: loss improved from 1.26957 to 1.26813, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7004 - loss: 1.2637\n",
            "Epoch 315/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7070 - loss: 1.2023 \n",
            "Epoch 315: loss improved from 1.26813 to 1.26530, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6971 - loss: 1.2246\n",
            "Epoch 316/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7150 - loss: 1.2144  \n",
            "Epoch 316: loss improved from 1.26530 to 1.26170, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7100 - loss: 1.2276\n",
            "Epoch 317/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6968 - loss: 1.2272 \n",
            "Epoch 317: loss improved from 1.26170 to 1.25870, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6922 - loss: 1.2349\n",
            "Epoch 318/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7126 - loss: 1.2242 \n",
            "Epoch 318: loss improved from 1.25870 to 1.25348, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7049 - loss: 1.2326\n",
            "Epoch 319/400\n",
            "\u001b[1m 8/17\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7325 - loss: 1.1705 \n",
            "Epoch 319: loss improved from 1.25348 to 1.24865, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7145 - loss: 1.2094\n",
            "Epoch 320/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7060 - loss: 1.2739 \n",
            "Epoch 320: loss improved from 1.24865 to 1.24807, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6982 - loss: 1.2617\n",
            "Epoch 321/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7263 - loss: 1.2102  \n",
            "Epoch 321: loss improved from 1.24807 to 1.24314, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7180 - loss: 1.2199\n",
            "Epoch 322/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6861 - loss: 1.2391 \n",
            "Epoch 322: loss improved from 1.24314 to 1.24158, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6876 - loss: 1.2384\n",
            "Epoch 323/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7043 - loss: 1.2518 \n",
            "Epoch 323: loss improved from 1.24158 to 1.23463, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7039 - loss: 1.2476\n",
            "Epoch 324/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7369 - loss: 1.1973 \n",
            "Epoch 324: loss improved from 1.23463 to 1.23311, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7208 - loss: 1.2100\n",
            "Epoch 325/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6987 - loss: 1.2626 \n",
            "Epoch 325: loss improved from 1.23311 to 1.23087, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6968 - loss: 1.2458\n",
            "Epoch 326/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7377 - loss: 1.1881  \n",
            "Epoch 326: loss improved from 1.23087 to 1.22666, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7266 - loss: 1.1961\n",
            "Epoch 327/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6937 - loss: 1.2492 \n",
            "Epoch 327: loss improved from 1.22666 to 1.22310, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6946 - loss: 1.2401\n",
            "Epoch 328/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6845 - loss: 1.2917 \n",
            "Epoch 328: loss improved from 1.22310 to 1.21958, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6927 - loss: 1.2667\n",
            "Epoch 329/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6538 - loss: 1.2739 \n",
            "Epoch 329: loss improved from 1.21958 to 1.21664, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6658 - loss: 1.2574\n",
            "Epoch 330/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7304 - loss: 1.1928 \n",
            "Epoch 330: loss improved from 1.21664 to 1.21179, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7189 - loss: 1.2061\n",
            "Epoch 331/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7282 - loss: 1.1882\n",
            "Epoch 331: loss improved from 1.21179 to 1.20779, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7256 - loss: 1.1904\n",
            "Epoch 332/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7112 - loss: 1.2014\n",
            "Epoch 332: loss improved from 1.20779 to 1.20573, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7101 - loss: 1.2019\n",
            "Epoch 333/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6950 - loss: 1.1825\n",
            "Epoch 333: loss improved from 1.20573 to 1.20073, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6953 - loss: 1.1835\n",
            "Epoch 334/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7116 - loss: 1.2240\n",
            "Epoch 334: loss improved from 1.20073 to 1.19774, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7114 - loss: 1.2211\n",
            "Epoch 335/400\n",
            "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7447 - loss: 1.1714\n",
            "Epoch 335: loss improved from 1.19774 to 1.19433, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7377 - loss: 1.1752\n",
            "Epoch 336/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6948 - loss: 1.1786\n",
            "Epoch 336: loss improved from 1.19433 to 1.19082, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6957 - loss: 1.1800\n",
            "Epoch 337/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7280 - loss: 1.1319\n",
            "Epoch 337: loss improved from 1.19082 to 1.18919, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7251 - loss: 1.1382\n",
            "Epoch 338/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7124 - loss: 1.1779\n",
            "Epoch 338: loss improved from 1.18919 to 1.18621, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7092 - loss: 1.1804\n",
            "Epoch 339/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7242 - loss: 1.1750\n",
            "Epoch 339: loss improved from 1.18621 to 1.18133, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7195 - loss: 1.1767\n",
            "Epoch 340/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7025 - loss: 1.2170\n",
            "Epoch 340: loss improved from 1.18133 to 1.18065, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7002 - loss: 1.2106\n",
            "Epoch 341/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7131 - loss: 1.1890  \n",
            "Epoch 341: loss improved from 1.18065 to 1.17482, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7108 - loss: 1.1744\n",
            "Epoch 342/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7026 - loss: 1.1665  \n",
            "Epoch 342: loss improved from 1.17482 to 1.17329, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7043 - loss: 1.1708\n",
            "Epoch 343/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7358 - loss: 1.1172  \n",
            "Epoch 343: loss improved from 1.17329 to 1.16824, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7274 - loss: 1.1327\n",
            "Epoch 344/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6886 - loss: 1.2206 \n",
            "Epoch 344: loss improved from 1.16824 to 1.16751, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6959 - loss: 1.2026\n",
            "Epoch 345/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7407 - loss: 1.1180 \n",
            "Epoch 345: loss improved from 1.16751 to 1.16290, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7332 - loss: 1.1288\n",
            "Epoch 346/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6983 - loss: 1.1848 \n",
            "Epoch 346: loss improved from 1.16290 to 1.15779, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7004 - loss: 1.1774\n",
            "Epoch 347/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7209 - loss: 1.1569 \n",
            "Epoch 347: loss improved from 1.15779 to 1.15516, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7168 - loss: 1.1584\n",
            "Epoch 348/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7345 - loss: 1.1133 \n",
            "Epoch 348: loss improved from 1.15516 to 1.15196, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7258 - loss: 1.1246\n",
            "Epoch 349/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7116 - loss: 1.1320 \n",
            "Epoch 349: loss improved from 1.15196 to 1.14892, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7122 - loss: 1.1416\n",
            "Epoch 350/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6978 - loss: 1.1851 \n",
            "Epoch 350: loss did not improve from 1.14892\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7003 - loss: 1.1739\n",
            "Epoch 351/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7077 - loss: 1.1568 \n",
            "Epoch 351: loss improved from 1.14892 to 1.14227, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7096 - loss: 1.1536\n",
            "Epoch 352/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7172 - loss: 1.1350 \n",
            "Epoch 352: loss improved from 1.14227 to 1.13965, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7135 - loss: 1.1408\n",
            "Epoch 353/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6764 - loss: 1.2065 \n",
            "Epoch 353: loss improved from 1.13965 to 1.13917, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6873 - loss: 1.1845\n",
            "Epoch 354/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7275 - loss: 1.1157 \n",
            "Epoch 354: loss improved from 1.13917 to 1.13655, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7206 - loss: 1.1230\n",
            "Epoch 355/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7630 - loss: 1.0835 \n",
            "Epoch 355: loss improved from 1.13655 to 1.13076, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7460 - loss: 1.1042\n",
            "Epoch 356/400\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7301 - loss: 1.1188\n",
            "Epoch 356: loss improved from 1.13076 to 1.12672, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7301 - loss: 1.1192\n",
            "Epoch 357/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7443 - loss: 1.1125 \n",
            "Epoch 357: loss improved from 1.12672 to 1.12540, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7381 - loss: 1.1151\n",
            "Epoch 358/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7258 - loss: 1.2122 \n",
            "Epoch 358: loss improved from 1.12540 to 1.12075, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7266 - loss: 1.1821\n",
            "Epoch 359/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7134 - loss: 1.1144 \n",
            "Epoch 359: loss did not improve from 1.12075\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7168 - loss: 1.1172\n",
            "Epoch 360/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7535 - loss: 1.0814 \n",
            "Epoch 360: loss improved from 1.12075 to 1.11974, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7454 - loss: 1.0913\n",
            "Epoch 361/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 1.1312 \n",
            "Epoch 361: loss improved from 1.11974 to 1.11319, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7205 - loss: 1.1227\n",
            "Epoch 362/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7262 - loss: 1.0770 \n",
            "Epoch 362: loss improved from 1.11319 to 1.11066, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7274 - loss: 1.0898\n",
            "Epoch 363/400\n",
            "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7804 - loss: 1.0706 \n",
            "Epoch 363: loss improved from 1.11066 to 1.10650, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7600 - loss: 1.0915\n",
            "Epoch 364/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7441 - loss: 1.1002 \n",
            "Epoch 364: loss improved from 1.10650 to 1.10367, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 1.1019\n",
            "Epoch 365/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7289 - loss: 1.1180 \n",
            "Epoch 365: loss improved from 1.10367 to 1.09967, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7289 - loss: 1.1150\n",
            "Epoch 366/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7725 - loss: 1.0383 \n",
            "Epoch 366: loss improved from 1.09967 to 1.09868, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7617 - loss: 1.0555\n",
            "Epoch 367/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7485 - loss: 1.0612 \n",
            "Epoch 367: loss improved from 1.09868 to 1.09653, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7442 - loss: 1.0733\n",
            "Epoch 368/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7438 - loss: 1.1189 \n",
            "Epoch 368: loss improved from 1.09653 to 1.09584, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7410 - loss: 1.1117\n",
            "Epoch 369/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7208 - loss: 1.1008  \n",
            "Epoch 369: loss improved from 1.09584 to 1.09072, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7272 - loss: 1.0959\n",
            "Epoch 370/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7604 - loss: 1.0246 \n",
            "Epoch 370: loss improved from 1.09072 to 1.08755, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7508 - loss: 1.0463\n",
            "Epoch 371/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7376 - loss: 1.0486 \n",
            "Epoch 371: loss improved from 1.08755 to 1.08601, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7366 - loss: 1.0650\n",
            "Epoch 372/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7660 - loss: 0.9899 \n",
            "Epoch 372: loss improved from 1.08601 to 1.07872, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7518 - loss: 1.0197\n",
            "Epoch 373/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7560 - loss: 1.0562 \n",
            "Epoch 373: loss did not improve from 1.07872\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7502 - loss: 1.0614\n",
            "Epoch 374/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7500 - loss: 1.0523 \n",
            "Epoch 374: loss improved from 1.07872 to 1.07548, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7450 - loss: 1.0576\n",
            "Epoch 375/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7842 - loss: 0.9926 \n",
            "Epoch 375: loss improved from 1.07548 to 1.07415, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7667 - loss: 1.0209\n",
            "Epoch 376/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7769 - loss: 1.0477  \n",
            "Epoch 376: loss improved from 1.07415 to 1.06947, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7619 - loss: 1.0572\n",
            "Epoch 377/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7309 - loss: 1.1086 \n",
            "Epoch 377: loss improved from 1.06947 to 1.06684, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7354 - loss: 1.0945\n",
            "Epoch 378/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7898 - loss: 0.9426 \n",
            "Epoch 378: loss did not improve from 1.06684\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7767 - loss: 0.9757\n",
            "Epoch 379/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 1.0407 \n",
            "Epoch 379: loss improved from 1.06684 to 1.06395, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 1.0449\n",
            "Epoch 380/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7565 - loss: 1.0267 \n",
            "Epoch 380: loss improved from 1.06395 to 1.06111, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7536 - loss: 1.0349\n",
            "Epoch 381/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7417 - loss: 1.0375 \n",
            "Epoch 381: loss improved from 1.06111 to 1.05617, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7379 - loss: 1.0513\n",
            "Epoch 382/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7874 - loss: 1.0150 \n",
            "Epoch 382: loss improved from 1.05617 to 1.05372, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7701 - loss: 1.0328\n",
            "Epoch 383/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 1.0584 \n",
            "Epoch 383: loss improved from 1.05372 to 1.05144, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7492 - loss: 1.0554\n",
            "Epoch 384/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7624 - loss: 1.0017 \n",
            "Epoch 384: loss improved from 1.05144 to 1.04926, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7576 - loss: 1.0138\n",
            "Epoch 385/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7521 - loss: 1.0367 \n",
            "Epoch 385: loss improved from 1.04926 to 1.04618, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7496 - loss: 1.0380\n",
            "Epoch 386/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7386 - loss: 1.0150  \n",
            "Epoch 386: loss improved from 1.04618 to 1.04560, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7427 - loss: 1.0258\n",
            "Epoch 387/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7594 - loss: 1.0240 \n",
            "Epoch 387: loss improved from 1.04560 to 1.04418, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7532 - loss: 1.0329\n",
            "Epoch 388/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7379 - loss: 1.0316  \n",
            "Epoch 388: loss improved from 1.04418 to 1.04041, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7399 - loss: 1.0319\n",
            "Epoch 389/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7796 - loss: 1.0253  \n",
            "Epoch 389: loss improved from 1.04041 to 1.03282, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7678 - loss: 1.0294\n",
            "Epoch 390/400\n",
            "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7797 - loss: 0.9658 \n",
            "Epoch 390: loss improved from 1.03282 to 1.03083, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7721 - loss: 0.9831\n",
            "Epoch 391/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7322 - loss: 1.0449  \n",
            "Epoch 391: loss improved from 1.03083 to 1.02931, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7345 - loss: 1.0405\n",
            "Epoch 392/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7609 - loss: 1.0573  \n",
            "Epoch 392: loss improved from 1.02931 to 1.02744, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7590 - loss: 1.0479\n",
            "Epoch 393/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7271 - loss: 1.0609 \n",
            "Epoch 393: loss improved from 1.02744 to 1.02727, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 1.0485\n",
            "Epoch 394/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8128 - loss: 0.9224 \n",
            "Epoch 394: loss improved from 1.02727 to 1.02249, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7915 - loss: 0.9573\n",
            "Epoch 395/400\n",
            "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7300 - loss: 1.0538 \n",
            "Epoch 395: loss improved from 1.02249 to 1.02054, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7382 - loss: 1.0425\n",
            "Epoch 396/400\n",
            "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7638 - loss: 0.9673  \n",
            "Epoch 396: loss improved from 1.02054 to 1.01420, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7623 - loss: 0.9860\n",
            "Epoch 397/400\n",
            "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7534 - loss: 1.0216  \n",
            "Epoch 397: loss did not improve from 1.01420\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7546 - loss: 1.0175\n",
            "Epoch 398/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7800 - loss: 0.9678\n",
            "Epoch 398: loss did not improve from 1.01420\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7746 - loss: 0.9780\n",
            "Epoch 399/400\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7465 - loss: 1.0101\n",
            "Epoch 399: loss improved from 1.01420 to 1.01272, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7459 - loss: 1.0104\n",
            "Epoch 400/400\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7560 - loss: 0.9871\n",
            "Epoch 400: loss improved from 1.01272 to 1.00766, saving model to next_words_gru.keras\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7544 - loss: 0.9910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7feb88846740>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GRU model from the checkpoint\n",
        "gru_model = load_model(\"next_words_gru.keras\")\n",
        "\n",
        "# Evaluate the GRU model on the test data\n",
        "gru_test_loss, gru_test_accuracy = gru_model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the test accuracy for the GRU model\n",
        "print(\"GRU Test Accuracy:\", gru_test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL2QZT1C9lVb",
        "outputId": "03c730f0-6aaa-45db-8596-fbb175f8a6cc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0438 - loss: 10.4576  \n",
            "GRU Test Accuracy: 0.04794520512223244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "model = load_model('next_words_gru.keras')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "\n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "REKp_jcz9sbG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "\n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "\n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "\n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B967lYcH92QC",
        "outputId": "2040cf62-4fee-464e-f31e-02e928342838"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your line: One day, an announcement was made throughout the\n",
            "['made', 'throughout', 'the']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "kingdom\n",
            "Enter your line: Cinderella also wished to go to the\n",
            "['go', 'to', 'the']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "ball\n",
            "Enter your line: Cinderella worked hard all day, finishing every task her stepmother\n",
            "['task', 'her', 'stepmother']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "gave\n",
            "Enter your line: The Fairy Godmother smiled warmly at Cinderella and\n",
            "['at', 'Cinderella', 'and']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "said\n",
            "Enter your line: But when her stepsisters saw her in the dress, they tore it to \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7feb881e9990> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', 'to', '']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "the\n",
            "Enter your line: Cinderella was hopeful as she put on the \n",
            "['on', 'the', '']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "tiny\n"
          ]
        }
      ]
    }
  ]
}